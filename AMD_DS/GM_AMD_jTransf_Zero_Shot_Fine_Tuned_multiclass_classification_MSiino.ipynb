{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/GM_SOURCE_CODE/blob/main/AMD_DS/GM_AMD_jTransf_Zero_Shot_Fine_Tuned_multiclass_classification_MSiino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4DW7kiug94o"
      },
      "source": [
        "# Fine Tuning Transformer for MultiClass Text Classification of Source Code. Notebook by Marco Siino et al."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJOVl9vLg94q"
      },
      "source": [
        "# Importing Python Libraries and preparing the environment\n",
        "\n",
        "At this step we will be importing the libraries and modules needed to run our script. Libraries are:\n",
        "* Pandas\n",
        "* Pytorch\n",
        "* Pytorch Utils for Dataset and Dataloader\n",
        "* Transformers\n",
        "* BERT Model and Tokenizer\n",
        "\n",
        "Followed by that we will preapre the device for CUDA execeution. This configuration is needed if you want to leverage on onboard GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wuMlXT80GAMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddaf6eee-fb8a-49ef-8628-832c756c3c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Importing the libraries needed\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForMaskedLM, BertModel, DistilBertModel, DistilBertTokenizer\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xQMKTZ4ARk12"
      },
      "outputs": [],
      "source": [
        "# Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNOPbPvVg94s"
      },
      "source": [
        "# Importing and Pre-Processing the domain data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iNCaZ2epNcSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83055f9a-614b-4cb1-e0be-5d1da9a7fd29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   src oracle  ENCODE_CAT\n",
            "0    2048 - 255 - __kernel void A(int a, const __gl...    GPU           1\n",
            "1    131072 - 256 - __kernel void A(__global uint* ...    CPU           0\n",
            "2    3145728 - 256 - extern void B(float4 a, float4...    GPU           1\n",
            "3    4096 - 256 - __kernel void A(__global float* a...    CPU           0\n",
            "4    524288 - 256 - __kernel void A(__global uint* ...    CPU           0\n",
            "..                                                 ...    ...         ...\n",
            "675  2000628 - 128 - __kernel void A(__global const...    CPU           0\n",
            "676  2000628 - 128 - __kernel void A(__global const...    CPU           0\n",
            "677  71647488 - 0 - extern int D(__private int, __p...    GPU           1\n",
            "678  71647488 - 256 - extern int B(int, int, int);\\...    GPU           1\n",
            "679  117440512 - 128 - __kernel void A(__global con...    CPU           0\n",
            "\n",
            "[680 rows x 3 columns]\n",
            "                                                   src oracle  ENCODE_CAT\n",
            "0    6346800 - 52 - __kernel void A(__global double...    CPU           0\n",
            "1    15948384 - 48 - __kernel void A(__global const...    CPU           0\n",
            "2    14742140 - 128 - extern double __clc_pow(const...    CPU           0\n",
            "3    15948384 - 2 - extern void B(double m, double ...    CPU           0\n",
            "4    755866968 - 64 - __kernel void A(__global doub...    CPU           0\n",
            "..                                                 ...    ...         ...\n",
            "675  14742140 - 128 - __kernel void A(__global doub...    CPU           0\n",
            "676  57594120 - 128 - __kernel void A(__global cons...    CPU           0\n",
            "677  320457304 - 64 - extern void B(double, double,...    GPU           1\n",
            "678  1344811536 - 20 - extern void D(__private doub...    GPU           1\n",
            "679  1313675032 - 32 - __kernel void A(__global con...    GPU           1\n",
            "\n",
            "[680 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# Import the csv into pandas dataframe and add the headers\n",
        "\n",
        "df = pd.read_csv('dataset-devmap-amd.csv',sep=',')\n",
        "#df = pd.concat(map(pd.read_csv, ['dataset-devmap-nvidia.csv', 'dataset-devmap-amd.csv']))\n",
        "#df = pd.read_csv('dataset-devmap-nvidia.csv', sep='\\t', names=['benchmark','dataset','comp','rational','mem','localmem','coalesced','atomic','transfer','wgsize','oracle','runtime_cpu','runtime_gpu','src','seq'])\n",
        "#print(df.head())\n",
        "# Now include transfer and wgsize columns into the src column.\n",
        "df['src'] = df['transfer'].astype(str) +\" - \"+ df['wgsize'].astype(str) +\" - \"+df[\"src\"]\n",
        "# # Removing unwanted columns and only leaving title of news and the category which will be the target\n",
        "df = df[['src','oracle']]\n",
        "#print(df.head())\n",
        "\n",
        "encode_dict = {}\n",
        "\n",
        "def encode_cat(x):\n",
        "    if x == \"GPU\":\n",
        "      encode_dict[x]=1\n",
        "    else:\n",
        "      encode_dict[x]=0\n",
        "    return encode_dict[x]\n",
        "\n",
        "df['ENCODE_CAT'] = df['oracle'].apply(lambda x: encode_cat(x))\n",
        "\n",
        "print(df)\n",
        "\n",
        "df = df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDomDFc5g94s"
      },
      "source": [
        "# Preparing the Dataset and Dataloader\n",
        "\n",
        "We will start with defining few key variables that will be used later during the training/fine tuning stage.\n",
        "Followed by creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. We will also define the Dataloader that will feed  the data in batches to the neural network for suitable training and processing.\n",
        "Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "#### *Triage* Dataset Class\n",
        "- This class is defined to accept the Dataframe as input and generate tokenized output that is used by the DistilBERT model for training.\n",
        "- We are using the DistilBERT tokenizer to tokenize the data in the `TITLE` column of the dataframe.\n",
        "- The tokenizer uses the `encode_plus` method to perform tokenization and generate the necessary outputs, namely: `ids`, `attention_mask`\n",
        "- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer)\n",
        "- `target` is the encoded category on the news headline.\n",
        "- The *Triage* class is used to create 2 datasets, for training and for validation.\n",
        "- *Training Dataset* is used to fine tune the model: **80% of the original data**\n",
        "- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training.\n",
        "\n",
        "#### Dataloader\n",
        "- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n",
        "- This control is achieved using the parameters such as `batch_size` and `max_len`.\n",
        "- Training and Validation dataloaders are used in the training and validation part of the flow respectively"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RIMPIAZZO MCROCK\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = BertTokenizer.from_pretrained('jTrans/jtrans_tokenizer')"
      ],
      "metadata": {
        "id": "JhA_AVEdrxWF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2vX7kzaAHu39"
      },
      "outputs": [],
      "source": [
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.data.src[index])\n",
        "        title = \" \".join(title.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSRhkzXKg94u"
      },
      "source": [
        "# Creating the Transformer for Fine Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model.\n",
        "\n",
        "class BinBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinBERTClass, self).__init__()\n",
        "        self.l1 = BinBertModel.from_pretrained('jTrans/models/jTrans-finetune')\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "\n",
        "class BinBertModel(BertModel):\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "        self.embeddings.position_embeddings=self.embeddings.word_embeddings"
      ],
      "metadata": {
        "id": "JYdlFn3z114Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate the 5 fold objects."
      ],
      "metadata": {
        "id": "L5GIoLhYyY79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Each i-train fold can be accessed with df_train[i]. Same for test.\n",
        "fold_nr = 5\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=None, shuffle=False)\n",
        "\n",
        "df_train = []\n",
        "df_test = []\n",
        "model = []\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(kf.split(df)):\n",
        "  df_train.append(df.iloc[train_index])\n",
        "  df_test.append(df.iloc[test_index])\n",
        "# print(df_train[0])\n",
        "\n",
        "for i in range(0,fold_nr):\n",
        "  df_train[i] = df_train[i].reset_index(drop=True)\n",
        "  df_test[i] = df_test[i].reset_index(drop=True)\n",
        "  # Generate a different model for each fold.\n",
        "  model.append(BinBERTClass())\n",
        "  model[i].to(device)\n"
      ],
      "metadata": {
        "id": "Dx0eqM-Gynq4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Zcwq13c0NE9c"
      },
      "outputs": [],
      "source": [
        "# Creating the dataset and dataloader\n",
        "\n",
        "#train_size = 0.8\n",
        "#train_dataset=df.sample(frac=train_size,random_state=200)\n",
        "#test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "#train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "#print(\"FULL Dataset: {}\".format(df.shape))\n",
        "#print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "#print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "#training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "#testing_set = Triage(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "training_set = []\n",
        "testing_set = []\n",
        "training_loader = []\n",
        "testing_loader = []\n",
        "\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "\n",
        "for i in range(0,fold_nr):\n",
        "  training_set.append(Triage(df_train[i], tokenizer, MAX_LEN))\n",
        "  testing_set.append(Triage(df_test[i], tokenizer, MAX_LEN))\n",
        "  training_loader.append(DataLoader(training_set[i], **train_params))\n",
        "  testing_loader.append(DataLoader(testing_set[i], **test_params))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate accuracy"
      ],
      "metadata": {
        "id": "E6vJBtdSMOC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U3U4s3e9g94u"
      },
      "outputs": [],
      "source": [
        "# Function to calcuate the accuracy of the model\n",
        "\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fF3XpswUg94u"
      },
      "outputs": [],
      "source": [
        "# Creating the loss function and optimizer\n",
        "optimizer = []\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "for i in range(0,fold_nr):\n",
        "  optimizer.append(torch.optim.Adam(params =  model[i].parameters(), lr=LEARNING_RATE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBcni6oig94u"
      },
      "source": [
        "# Define the Fine Tuning of the Model\n",
        "\n",
        "After all the effort of loading and preparing the data and datasets, creating the model and defining its loss and optimizer. This is probably the easier steps in the process.\n",
        "\n",
        "Here we define a training function that trains the model on the training dataset created above, specified number of times (EPOCH), An epoch defines how many times the complete data will be passed through the network.\n",
        "\n",
        "Following events happen in this function to fine tune the neural network:\n",
        "- The dataloader passes data to the model based on the batch size.\n",
        "- Subsequent output from the model and the actual category are compared to calculate the loss.\n",
        "- Loss value is used to optimize the weights of the neurons in the network.\n",
        "- After every 5000 steps the loss value is printed in the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QYzhifllg94u"
      },
      "outputs": [],
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch, model,optimizer, training_loader):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "\n",
        "        if _%5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples\n",
        "            print(f\"\\nTraining Loss per 5000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-Shot evaluation on the 5 fold."
      ],
      "metadata": {
        "id": "zNMpIcGYTyu_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KzL51A5qg94v"
      },
      "outputs": [],
      "source": [
        "\n",
        "def valid(model, testing_loader):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            #print(\"\\n\\nSTEP Nr. \", nb_tr_steps)\n",
        "            # Validation batch is 2. Then, every step is 2 predictions. Then the total steps are half of the size of the test fold.\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            #print(\"ids è\", ids)\n",
        "            #print(\"mask è\", mask)\n",
        "            outputs = model(ids, mask).squeeze()\n",
        "            #print(\"gli outputs sono:\",outputs)\n",
        "            #print(\"i targets sono:\",targets)\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "            #print(\"bid_idx è\",big_idx)\n",
        "            #print(\"Correct now is:\", n_correct)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "\n",
        "            if _%5000==0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct*100)/nb_tr_examples\n",
        "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return epoch_accu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-xBmWV93g94v",
        "outputId": "6ab0c197-7689-43ed-e7f8-30f171a875f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the validation section to print the accuracy and see how it performs\n",
            "Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch\n",
            "\n",
            "Entering FOLD NR.  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/leonardo/home/userexternal/msiino00/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss per 100 steps: 0.7322238087654114\n",
            "Validation Accuracy per 100 steps: 50.0\n",
            "Validation Loss Epoch: 0.6972496343009612\n",
            "Validation Accuracy Epoch: 58.088235294117645\n",
            "Accuracy on test data = 58.09%\n",
            "\n",
            "Entering FOLD NR.  1\n",
            "Validation Loss per 100 steps: 0.8587867617607117\n",
            "Validation Accuracy per 100 steps: 0.0\n",
            "Validation Loss Epoch: 0.7255997088025598\n",
            "Validation Accuracy Epoch: 44.11764705882353\n",
            "Accuracy on test data = 44.12%\n",
            "\n",
            "Entering FOLD NR.  2\n",
            "Validation Loss per 100 steps: 0.709701418876648\n",
            "Validation Accuracy per 100 steps: 0.0\n",
            "Validation Loss Epoch: 0.6948016610215692\n",
            "Validation Accuracy Epoch: 47.05882352941177\n",
            "Accuracy on test data = 47.06%\n",
            "\n",
            "Entering FOLD NR.  3\n",
            "Validation Loss per 100 steps: 0.4147166311740875\n",
            "Validation Accuracy per 100 steps: 100.0\n",
            "Validation Loss Epoch: 0.6742119552458034\n",
            "Validation Accuracy Epoch: 61.029411764705884\n",
            "Accuracy on test data = 61.03%\n",
            "\n",
            "Entering FOLD NR.  4\n",
            "Validation Loss per 100 steps: 0.7063864469528198\n",
            "Validation Accuracy per 100 steps: 50.0\n",
            "Validation Loss Epoch: 0.7354859885047463\n",
            "Validation Accuracy Epoch: 42.64705882352941\n",
            "Accuracy on test data = 42.65%\n"
          ]
        }
      ],
      "source": [
        "print('This is the validation section to print the accuracy and see how it performs')\n",
        "print('Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch')\n",
        "for i in range(0,fold_nr):\n",
        "  print(\"\\nEntering FOLD NR. \", i)\n",
        "  acc = valid(model[i], testing_loader[i])\n",
        "  print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning the model."
      ],
      "metadata": {
        "id": "lX5GVxBdUHtr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZMfM4AVNg94v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a17a1a-1c49-4cb5-d297-e44c619d8297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entering FOLD NR.  0\n",
            "\n",
            "Training Loss per 5000 steps: 0.6905779838562012\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 0: 57.169117647058826\n",
            "Training Loss Epoch: 0.686575446935261\n",
            "Training Accuracy Epoch: 57.169117647058826\n",
            "\n",
            "Training Loss per 5000 steps: 0.6875824332237244\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 1: 51.838235294117645\n",
            "Training Loss Epoch: 0.6990635966991677\n",
            "Training Accuracy Epoch: 51.838235294117645\n",
            "\n",
            "Training Loss per 5000 steps: 0.7073158621788025\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 2: 56.61764705882353\n",
            "Training Loss Epoch: 0.6957227951463532\n",
            "Training Accuracy Epoch: 56.61764705882353\n",
            "\n",
            "Training Loss per 5000 steps: 0.7638786435127258\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 3: 56.61764705882353\n",
            "Training Loss Epoch: 0.6895691934315598\n",
            "Training Accuracy Epoch: 56.61764705882353\n",
            "\n",
            "Training Loss per 5000 steps: 0.597946286201477\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 4: 56.25\n",
            "Training Loss Epoch: 0.7029101666720474\n",
            "Training Accuracy Epoch: 56.25\n",
            "\n",
            "Training Loss per 5000 steps: 0.786257803440094\n",
            "Training Accuracy per 5000 steps: 0.0\n",
            "The Total Accuracy for Epoch 5: 55.14705882352941\n",
            "Training Loss Epoch: 0.6899086464853847\n",
            "Training Accuracy Epoch: 55.14705882352941\n",
            "\n",
            "Training Loss per 5000 steps: 0.8375033140182495\n",
            "Training Accuracy per 5000 steps: 25.0\n",
            "The Total Accuracy for Epoch 6: 56.43382352941177\n",
            "Training Loss Epoch: 0.6941884791149813\n",
            "Training Accuracy Epoch: 56.43382352941177\n",
            "\n",
            "Training Loss per 5000 steps: 0.6686376333236694\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 7: 55.14705882352941\n",
            "Training Loss Epoch: 0.6872394794926924\n",
            "Training Accuracy Epoch: 55.14705882352941\n",
            "\n",
            "Training Loss per 5000 steps: 0.4236862063407898\n",
            "Training Accuracy per 5000 steps: 100.0\n",
            "The Total Accuracy for Epoch 8: 56.06617647058823\n",
            "Training Loss Epoch: 0.6898464663502049\n",
            "Training Accuracy Epoch: 56.06617647058823\n",
            "\n",
            "Training Loss per 5000 steps: 0.5711987614631653\n",
            "Training Accuracy per 5000 steps: 100.0\n",
            "The Total Accuracy for Epoch 9: 58.088235294117645\n",
            "Training Loss Epoch: 0.6886523144648355\n",
            "Training Accuracy Epoch: 58.088235294117645\n",
            "\n",
            "Entering FOLD NR.  1\n",
            "\n",
            "Training Loss per 5000 steps: 0.7687124013900757\n",
            "Training Accuracy per 5000 steps: 0.0\n",
            "The Total Accuracy for Epoch 0: 55.6985294117647\n",
            "Training Loss Epoch: 0.6953521986656329\n",
            "Training Accuracy Epoch: 55.6985294117647\n",
            "\n",
            "Training Loss per 5000 steps: 0.8115480542182922\n",
            "Training Accuracy per 5000 steps: 0.0\n",
            "The Total Accuracy for Epoch 1: 63.0514705882353\n",
            "Training Loss Epoch: 0.6537353646229295\n",
            "Training Accuracy Epoch: 63.0514705882353\n",
            "\n",
            "Training Loss per 5000 steps: 0.6004648208618164\n",
            "Training Accuracy per 5000 steps: 100.0\n",
            "The Total Accuracy for Epoch 2: 65.07352941176471\n",
            "Training Loss Epoch: 0.653616251752657\n",
            "Training Accuracy Epoch: 65.07352941176471\n",
            "\n",
            "Training Loss per 5000 steps: 0.9785215854644775\n",
            "Training Accuracy per 5000 steps: 0.0\n",
            "The Total Accuracy for Epoch 3: 65.25735294117646\n",
            "Training Loss Epoch: 0.6569129009457195\n",
            "Training Accuracy Epoch: 65.25735294117646\n",
            "\n",
            "Training Loss per 5000 steps: 0.7365692853927612\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 4: 64.88970588235294\n",
            "Training Loss Epoch: 0.6447779882480117\n",
            "Training Accuracy Epoch: 64.88970588235294\n",
            "\n",
            "Training Loss per 5000 steps: 0.34721627831459045\n",
            "Training Accuracy per 5000 steps: 100.0\n",
            "The Total Accuracy for Epoch 5: 65.80882352941177\n",
            "Training Loss Epoch: 0.6312939395361087\n",
            "Training Accuracy Epoch: 65.80882352941177\n",
            "\n",
            "Training Loss per 5000 steps: 0.731916069984436\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 6: 66.54411764705883\n",
            "Training Loss Epoch: 0.6058360283427379\n",
            "Training Accuracy Epoch: 66.54411764705883\n",
            "\n",
            "Training Loss per 5000 steps: 0.5335384011268616\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 7: 69.8529411764706\n",
            "Training Loss Epoch: 0.5761462473036612\n",
            "Training Accuracy Epoch: 69.8529411764706\n",
            "\n",
            "Training Loss per 5000 steps: 0.8050254583358765\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 8: 70.03676470588235\n",
            "Training Loss Epoch: 0.5793857138384791\n",
            "Training Accuracy Epoch: 70.03676470588235\n",
            "\n",
            "Training Loss per 5000 steps: 0.2471153736114502\n",
            "Training Accuracy per 5000 steps: 100.0\n",
            "The Total Accuracy for Epoch 9: 67.83088235294117\n",
            "Training Loss Epoch: 0.5778536739046959\n",
            "Training Accuracy Epoch: 67.83088235294117\n",
            "\n",
            "Entering FOLD NR.  2\n",
            "\n",
            "Training Loss per 5000 steps: 0.7640719413757324\n",
            "Training Accuracy per 5000 steps: 0.0\n",
            "The Total Accuracy for Epoch 0: 57.536764705882355\n",
            "Training Loss Epoch: 0.6885930139790563\n",
            "Training Accuracy Epoch: 57.536764705882355\n",
            "\n",
            "Training Loss per 5000 steps: 0.6690161824226379\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 1: 58.088235294117645\n",
            "Training Loss Epoch: 0.6883464718566221\n",
            "Training Accuracy Epoch: 58.088235294117645\n",
            "\n",
            "Training Loss per 5000 steps: 0.5892682075500488\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 2: 56.98529411764706\n",
            "Training Loss Epoch: 0.6871002103914233\n",
            "Training Accuracy Epoch: 56.98529411764706\n",
            "\n",
            "Training Loss per 5000 steps: 0.514503538608551\n",
            "Training Accuracy per 5000 steps: 100.0\n",
            "The Total Accuracy for Epoch 3: 59.375\n",
            "Training Loss Epoch: 0.6697429225725287\n",
            "Training Accuracy Epoch: 59.375\n",
            "\n",
            "Training Loss per 5000 steps: 0.7648942470550537\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 4: 62.5\n",
            "Training Loss Epoch: 0.670526069972445\n",
            "Training Accuracy Epoch: 62.5\n",
            "\n",
            "Training Loss per 5000 steps: 0.5163624286651611\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 5: 59.55882352941177\n",
            "Training Loss Epoch: 0.6743163058862967\n",
            "Training Accuracy Epoch: 59.55882352941177\n",
            "\n",
            "Training Loss per 5000 steps: 0.8165555596351624\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 6: 63.23529411764706\n",
            "Training Loss Epoch: 0.6618302878649795\n",
            "Training Accuracy Epoch: 63.23529411764706\n",
            "\n",
            "Training Loss per 5000 steps: 0.5694215297698975\n",
            "Training Accuracy per 5000 steps: 100.0\n",
            "The Total Accuracy for Epoch 7: 63.419117647058826\n",
            "Training Loss Epoch: 0.6522597383488627\n",
            "Training Accuracy Epoch: 63.419117647058826\n",
            "\n",
            "Training Loss per 5000 steps: 0.881952166557312\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 8: 64.5220588235294\n",
            "Training Loss Epoch: 0.6472276199389907\n",
            "Training Accuracy Epoch: 64.5220588235294\n",
            "\n",
            "Training Loss per 5000 steps: 0.5334447026252747\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 9: 64.33823529411765\n",
            "Training Loss Epoch: 0.6380764929687276\n",
            "Training Accuracy Epoch: 64.33823529411765\n",
            "\n",
            "Entering FOLD NR.  3\n",
            "\n",
            "Training Loss per 5000 steps: 1.013756513595581\n",
            "Training Accuracy per 5000 steps: 0.0\n",
            "The Total Accuracy for Epoch 0: 54.963235294117645\n",
            "Training Loss Epoch: 0.6990610512302202\n",
            "Training Accuracy Epoch: 54.963235294117645\n",
            "\n",
            "Training Loss per 5000 steps: 0.9873009324073792\n",
            "Training Accuracy per 5000 steps: 0.0\n",
            "The Total Accuracy for Epoch 1: 56.61764705882353\n",
            "Training Loss Epoch: 0.6892748102545738\n",
            "Training Accuracy Epoch: 56.61764705882353\n",
            "\n",
            "Training Loss per 5000 steps: 0.6676467657089233\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 2: 58.8235294117647\n",
            "Training Loss Epoch: 0.6828443927361685\n",
            "Training Accuracy Epoch: 58.8235294117647\n",
            "\n",
            "Training Loss per 5000 steps: 0.615462064743042\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 3: 61.213235294117645\n",
            "Training Loss Epoch: 0.6721801396240207\n",
            "Training Accuracy Epoch: 61.213235294117645\n",
            "\n",
            "Training Loss per 5000 steps: 0.9446125626564026\n",
            "Training Accuracy per 5000 steps: 25.0\n",
            "The Total Accuracy for Epoch 4: 59.55882352941177\n",
            "Training Loss Epoch: 0.6710421254529673\n",
            "Training Accuracy Epoch: 59.55882352941177\n",
            "\n",
            "Training Loss per 5000 steps: 0.9118322730064392\n",
            "Training Accuracy per 5000 steps: 25.0\n",
            "The Total Accuracy for Epoch 5: 61.9485294117647\n",
            "Training Loss Epoch: 0.6564302391865674\n",
            "Training Accuracy Epoch: 61.9485294117647\n",
            "\n",
            "Training Loss per 5000 steps: 0.6210047006607056\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 6: 61.580882352941174\n",
            "Training Loss Epoch: 0.6691516670672333\n",
            "Training Accuracy Epoch: 61.580882352941174\n",
            "\n",
            "Training Loss per 5000 steps: 0.6886222958564758\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 7: 61.213235294117645\n",
            "Training Loss Epoch: 0.6638743667041554\n",
            "Training Accuracy Epoch: 61.213235294117645\n",
            "\n",
            "Training Loss per 5000 steps: 0.6072120666503906\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 8: 63.60294117647059\n",
            "Training Loss Epoch: 0.6745697983047542\n",
            "Training Accuracy Epoch: 63.60294117647059\n",
            "\n",
            "Training Loss per 5000 steps: 0.6658256649971008\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 9: 63.23529411764706\n",
            "Training Loss Epoch: 0.6528959865955746\n",
            "Training Accuracy Epoch: 63.23529411764706\n",
            "\n",
            "Entering FOLD NR.  4\n",
            "\n",
            "Training Loss per 5000 steps: 0.7179512977600098\n",
            "Training Accuracy per 5000 steps: 25.0\n",
            "The Total Accuracy for Epoch 0: 56.06617647058823\n",
            "Training Loss Epoch: 0.6947143310133148\n",
            "Training Accuracy Epoch: 56.06617647058823\n",
            "\n",
            "Training Loss per 5000 steps: 0.810828685760498\n",
            "Training Accuracy per 5000 steps: 25.0\n",
            "The Total Accuracy for Epoch 1: 58.63970588235294\n",
            "Training Loss Epoch: 0.6812554229708279\n",
            "Training Accuracy Epoch: 58.63970588235294\n",
            "\n",
            "Training Loss per 5000 steps: 0.8781556487083435\n",
            "Training Accuracy per 5000 steps: 25.0\n",
            "The Total Accuracy for Epoch 2: 59.19117647058823\n",
            "Training Loss Epoch: 0.674244487329441\n",
            "Training Accuracy Epoch: 59.19117647058823\n",
            "\n",
            "Training Loss per 5000 steps: 0.5768951773643494\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 3: 58.088235294117645\n",
            "Training Loss Epoch: 0.6815968381569666\n",
            "Training Accuracy Epoch: 58.088235294117645\n",
            "\n",
            "Training Loss per 5000 steps: 0.7137047648429871\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 4: 57.536764705882355\n",
            "Training Loss Epoch: 0.6911239549517632\n",
            "Training Accuracy Epoch: 57.536764705882355\n",
            "\n",
            "Training Loss per 5000 steps: 0.5648928880691528\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 5: 57.904411764705884\n",
            "Training Loss Epoch: 0.6759733439368361\n",
            "Training Accuracy Epoch: 57.904411764705884\n",
            "\n",
            "Training Loss per 5000 steps: 0.856871485710144\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 6: 58.088235294117645\n",
            "Training Loss Epoch: 0.6742579849327311\n",
            "Training Accuracy Epoch: 58.088235294117645\n",
            "\n",
            "Training Loss per 5000 steps: 0.6109987497329712\n",
            "Training Accuracy per 5000 steps: 75.0\n",
            "The Total Accuracy for Epoch 7: 61.39705882352941\n",
            "Training Loss Epoch: 0.6584986143690699\n",
            "Training Accuracy Epoch: 61.39705882352941\n",
            "\n",
            "Training Loss per 5000 steps: 1.1719810962677002\n",
            "Training Accuracy per 5000 steps: 0.0\n",
            "The Total Accuracy for Epoch 8: 61.029411764705884\n",
            "Training Loss Epoch: 0.6649009767262375\n",
            "Training Accuracy Epoch: 61.029411764705884\n",
            "\n",
            "Training Loss per 5000 steps: 0.3899097740650177\n",
            "Training Accuracy per 5000 steps: 100.0\n",
            "The Total Accuracy for Epoch 9: 56.98529411764706\n",
            "Training Loss Epoch: 0.6755489078076446\n",
            "Training Accuracy Epoch: 56.98529411764706\n"
          ]
        }
      ],
      "source": [
        "for i in range(0,fold_nr):\n",
        "  print(\"\\nEntering FOLD NR. \", i)\n",
        "  for epoch in range(EPOCHS):\n",
        "    train(epoch,model[i],optimizer[i],training_loader[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the test set on the fine-tuned model."
      ],
      "metadata": {
        "id": "6_1zvnOBUXYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the validation section to print the accuracy and see how it performs')\n",
        "print('Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch')\n",
        "for i in range(0,fold_nr):\n",
        "  print(\"\\nEntering FOLD NR. \", i)\n",
        "  acc = valid(model[i], testing_loader[i])\n",
        "  print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ],
      "metadata": {
        "id": "-TqE0rGlUXBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56107c1c-ae89-4979-9d89-43d1ccb58fc5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the validation section to print the accuracy and see how it performs\n",
            "Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch\n",
            "\n",
            "Entering FOLD NR.  0\n",
            "Validation Loss per 100 steps: 0.5163393020629883\n",
            "Validation Accuracy per 100 steps: 100.0\n",
            "Validation Loss Epoch: 0.6571579391465467\n",
            "Validation Accuracy Epoch: 63.970588235294116\n",
            "Accuracy on test data = 63.97%\n",
            "\n",
            "Entering FOLD NR.  1\n",
            "Validation Loss per 100 steps: 0.5302715301513672\n",
            "Validation Accuracy per 100 steps: 100.0\n",
            "Validation Loss Epoch: 0.620403942597263\n",
            "Validation Accuracy Epoch: 59.55882352941177\n",
            "Accuracy on test data = 59.56%\n",
            "\n",
            "Entering FOLD NR.  2\n",
            "Validation Loss per 100 steps: 0.405139684677124\n",
            "Validation Accuracy per 100 steps: 100.0\n",
            "Validation Loss Epoch: 0.6662765053265235\n",
            "Validation Accuracy Epoch: 62.5\n",
            "Accuracy on test data = 62.50%\n",
            "\n",
            "Entering FOLD NR.  3\n",
            "Validation Loss per 100 steps: 0.5623577237129211\n",
            "Validation Accuracy per 100 steps: 100.0\n",
            "Validation Loss Epoch: 0.6310176134986036\n",
            "Validation Accuracy Epoch: 67.6470588235294\n",
            "Accuracy on test data = 67.65%\n",
            "\n",
            "Entering FOLD NR.  4\n",
            "Validation Loss per 100 steps: 0.871373176574707\n",
            "Validation Accuracy per 100 steps: 50.0\n",
            "Validation Loss Epoch: 0.6726101286271039\n",
            "Validation Accuracy Epoch: 57.35294117647059\n",
            "Accuracy on test data = 57.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLbwK-Mrg94v"
      },
      "source": [
        "# Saving the Trained Model Artifacts for inference\n",
        "\n",
        "This is the final step in the process of fine tuning the model.\n",
        "\n",
        "The model and its vocabulary are saved locally. These files are then used in the future to make inference on new inputs of news headlines.\n",
        "\n",
        "Please remember that a trained neural network is only useful when used in actual inference after its training.\n",
        "\n",
        "In the lifecycle of an ML projects this is only half the job done. We will leave the inference of these models for some other day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "492L-lBBg94v"
      },
      "outputs": [],
      "source": [
        "# Saving the files for re-use\n",
        "\n",
        "output_model_file = './models/pytorch_distilbert_news.bin'\n",
        "output_vocab_file = './models/vocab_distilbert_news.bin'\n",
        "\n",
        "model_to_save = model\n",
        "torch.save(model_to_save, output_model_file)\n",
        "tokenizer.save_vocabulary(output_vocab_file)\n",
        "\n",
        "print('All files saved')\n",
        "print('This tutorial is completed')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}